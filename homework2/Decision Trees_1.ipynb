{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "y3DiadTG-ru-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn \n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QpBS-Nd-fAO"
      },
      "source": [
        "# –î–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞: –¥–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZVCYFgU-5Tw"
      },
      "source": [
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç–µ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ü–µ–Ω—ã —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞ Azamon.\n",
        "\n",
        "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç–µ:\n",
        "- –í–æ –≤—Å–µ—Ö –≥—Ä–∞—Ñ–∏–∫–∞—Ö –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–¥–ø–∏—Å–∏ —á–µ—Ä–µ–∑ title, legend, etc.\n",
        "- –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ, —á—Ç–æ —É –≤–∞—Å –Ω–µ —Ç–µ–∫—É—Ç –¥–∞–Ω–Ω—ã–µ. –û–±—ã—á–Ω–æ —ç—Ç–æ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–µ, –Ω–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –æ—Ü–µ–Ω–∫—É üåö\n",
        "- –ï—Å–ª–∏ –≤—ã —Å–¥–∞–µ—Ç–µ —Ä–∞–±–æ—Ç—É –≤ Google Colaboratory, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–∞—à–∞ —Ç–µ—Ç—Ä–∞–¥–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ —Å—Å—ã–ª–∫–µ.\n",
        "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ–º–æ–≤ –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è, –Ω–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ–±–ª—é–¥–∞—Ç—å –º–µ—Ä—É. –ù–µ—Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞, —Å–æ—Å—Ç–æ—è—â–∞—è —Ç–æ–ª—å–∫–æ –∏–∑ –º–µ–º–æ–≤, –ø–æ–ª—É—á–∞–µ—Ç 0 –±–∞–ª–ª–æ–≤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyTAlXb8CbXk"
      },
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('amazon_co-ecommerce_sample.csv').drop(columns=[\n",
        "    'product_name',\n",
        "    'index',\n",
        "    'uniq_id',\n",
        "    'customers_who_bought_this_item_also_bought',\n",
        "    'items_customers_buy_after_viewing_this_item',\n",
        "    'sellers',\n",
        "    'description', # text\n",
        "    'product_information', # text\n",
        "    'product_description', # text\n",
        "    'customer_questions_and_answers', # text\n",
        "    'customer_reviews', # text\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>price</th>\n",
              "      <th>number_available_in_stock</th>\n",
              "      <th>number_of_reviews</th>\n",
              "      <th>number_of_answered_questions</th>\n",
              "      <th>average_review_rating</th>\n",
              "      <th>amazon_category_and_sub_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hornby</td>\n",
              "      <td>¬£3.42</td>\n",
              "      <td>5¬†new</td>\n",
              "      <td>15</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.9 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FunkyBuys</td>\n",
              "      <td>¬£16.99</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.5 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ccf</td>\n",
              "      <td>¬£9.99</td>\n",
              "      <td>2¬†new</td>\n",
              "      <td>17</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.9 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hornby</td>\n",
              "      <td>¬£39.99</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hornby</td>\n",
              "      <td>¬£32.19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.7 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  manufacturer   price number_available_in_stock number_of_reviews   \n",
              "0       Hornby   ¬£3.42                     5¬†new                15  \\\n",
              "1    FunkyBuys  ¬£16.99                       NaN                 2   \n",
              "2          ccf   ¬£9.99                     2¬†new                17   \n",
              "3       Hornby  ¬£39.99                       NaN                 1   \n",
              "4       Hornby  ¬£32.19                       NaN                 3   \n",
              "\n",
              "   number_of_answered_questions average_review_rating   \n",
              "0                           1.0    4.9 out of 5 stars  \\\n",
              "1                           1.0    4.5 out of 5 stars   \n",
              "2                           2.0    3.9 out of 5 stars   \n",
              "3                           2.0    5.0 out of 5 stars   \n",
              "4                           2.0    4.7 out of 5 stars   \n",
              "\n",
              "                    amazon_category_and_sub_category  \n",
              "0  Hobbies > Model Trains & Railway Sets > Rail V...  \n",
              "1  Hobbies > Model Trains & Railway Sets > Rail V...  \n",
              "2  Hobbies > Model Trains & Railway Sets > Rail V...  \n",
              "3  Hobbies > Model Trains & Railway Sets > Rail V...  \n",
              "4  Hobbies > Model Trains & Railway Sets > Rail V...  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O59k78A8C_yP"
      },
      "source": [
        "## –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (1 –±–∞–ª–ª)\n",
        "\n",
        "–ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏. –ï—Å—Ç—å –ª–∏ –≤ –Ω–∏—Ö –ø—Ä–æ–ø—É—Å–∫–∏? –ö–∞–∫–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –º–µ–∂–¥—É NaN'–∞–º–∏ –∏ –æ–±—â–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö? –ï—Å—Ç—å –ª–∏ —Å–º—ã—Å–ª –≤—ã–∫–∏–¥—ã–≤–∞—Ç—å –∫–∞–∫–∏–µ-–ª–∏–±–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "manufacturer                           7\n",
              "price                               1435\n",
              "number_available_in_stock           2500\n",
              "number_of_reviews                     18\n",
              "number_of_answered_questions         765\n",
              "average_review_rating                 18\n",
              "amazon_category_and_sub_category     690\n",
              "dtype: int64"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "manufacturer                        0.0007\n",
              "price                               0.1435\n",
              "number_available_in_stock           0.2500\n",
              "number_of_reviews                   0.0018\n",
              "number_of_answered_questions        0.0765\n",
              "average_review_rating               0.0018\n",
              "amazon_category_and_sub_category    0.0690\n",
              "dtype: float64"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum() / (df.notnull().sum() + df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>price</th>\n",
              "      <th>number_of_reviews</th>\n",
              "      <th>number_of_answered_questions</th>\n",
              "      <th>average_review_rating</th>\n",
              "      <th>amazon_category_and_sub_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hornby</td>\n",
              "      <td>¬£3.42</td>\n",
              "      <td>15</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.9 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FunkyBuys</td>\n",
              "      <td>¬£16.99</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.5 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ccf</td>\n",
              "      <td>¬£9.99</td>\n",
              "      <td>17</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.9 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hornby</td>\n",
              "      <td>¬£39.99</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hornby</td>\n",
              "      <td>¬£32.19</td>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.7 out of 5 stars</td>\n",
              "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  manufacturer   price number_of_reviews  number_of_answered_questions   \n",
              "0       Hornby   ¬£3.42                15                           1.0  \\\n",
              "1    FunkyBuys  ¬£16.99                 2                           1.0   \n",
              "2          ccf   ¬£9.99                17                           2.0   \n",
              "3       Hornby  ¬£39.99                 1                           2.0   \n",
              "4       Hornby  ¬£32.19                 3                           2.0   \n",
              "\n",
              "  average_review_rating                   amazon_category_and_sub_category  \n",
              "0    4.9 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...  \n",
              "1    4.5 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...  \n",
              "2    3.9 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...  \n",
              "3    5.0 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...  \n",
              "4    4.7 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...  "
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.drop(columns = ['number_available_in_stock'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH688KdZDdKY"
      },
      "source": [
        "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (3 –±–∞–ª–ª–∞)\n",
        "\n",
        "–û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥–ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ß—Ç–æ —ç—Ç–æ –∑–∞ –ø—Ä–∏–∑–Ω–∞–∫? –ó–∞–∫–æ–¥–∏—Ä—É–π—Ç–µ –∏ –µ–≥–æ.\n",
        "\n",
        "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã (+ 1 –±–∞–ª–ª):\n",
        "- –ö–∞–∫–∏–µ –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –ª—É—á—à–µ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ ordinal encoding?\n",
        "- –ö–∞–∫–∏–µ –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ–ø—É—Å—Ç–∏–º–æ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ one-hot?\n",
        "\n",
        "–ü—Ä–∏–º.: —Å—É–º–º–∞—Ä–Ω–æ –∑–∞ —ç—Ç—É —Å–µ–∫—Ü–∏—é –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –¥–æ 4 –±–∞–ª–ª–æ–≤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTRoUwANEcn9"
      },
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (3 –±–∞–ª–ª–∞)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df['price']\n",
        "X = df.drop(columns = ['price'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyVaKKT7FFo5"
      },
      "source": [
        "## –ë–µ–π–∑–ª–∞–π–Ω\n",
        "\n",
        "–û–±—É—á–∏—Ç–µ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `sklearn.dummy.DummyRegressor`. –ö–∞–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–µ—Å—Ç–µ? –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ MSE, RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: '¬£4.99'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[56], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m r2_score\n\u001b[1;32m----> 5\u001b[0m dummy_regr \u001b[39m=\u001b[39m DummyRegressor(strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      6\u001b[0m y_pred \u001b[39m=\u001b[39m dummy_regr\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m      7\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\dummy.py:540\u001b[0m, in \u001b[0;36mDummyRegressor.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the random regressor.\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \n\u001b[0;32m    522\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 540\u001b[0m y \u001b[39m=\u001b[39m check_array(y, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my must not be empty.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:917\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[39mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[39m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    916\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m--> 917\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(values, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m using_copy_on_write() \u001b[39mand\u001b[39;00m astype_is_view(values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m    919\u001b[0m     arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mview()\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '¬£4.99'"
          ]
        }
      ],
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "dummy_regr = DummyRegressor(strategy=\"mean\").fit(X_train, y_train)\n",
        "y_pred = dummy_regr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(mse)\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(rmse)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVW-MtPgF_vE"
      },
      "source": [
        "## –î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π\n",
        "\n",
        "–û–±—É—á–∏—Ç–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–µ –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –£–ª—É—á—à–∏–ª–æ—Å—å –ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é? –û—Ü–µ–Ω–∏—Ç–µ r2_score –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_boston\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m r2_score\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mload_boston\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    106\u001b[0m     msg \u001b[39m=\u001b[39m textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[0;32m    107\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mglobals\u001b[39m()[name]\n",
            "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import numpy as np\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "data = load_boston()\n",
        "\n",
        "X_train = data['data'][:400]\n",
        "y_train = data['target'][:400]\n",
        "\n",
        "X_test = data['data'][400:]\n",
        "y_test = data['target'][400:]\n",
        "\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# –†–∞—Å—á–µ—Ç r2_score –¥–ª—è –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π\n",
        "r2_dt = r2_score(y_test, y_pred_dt)\n",
        "\n",
        "print(\"R2 score –¥–ª—è –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π: {:.2f}\".format(r2_dt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viVY_kUMG7Jv"
      },
      "source": [
        "## –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n",
        "\n",
        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∏—Ç—å –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é. –û—Ü–µ–Ω–∏—Ç–µ r2_score –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –°—Ä–∞–≤–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –¥–µ—Ä–µ–≤–æ–º —Ä–µ—à–µ–Ω–∏–π. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_boston\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m r2_score\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mload_boston\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    106\u001b[0m     msg \u001b[39m=\u001b[39m textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[0;32m    107\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mglobals\u001b[39m()[name]\n",
            "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import numpy as np\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "data = load_boston()\n",
        "\n",
        "X_train = data['data'][:400]\n",
        "y_train = data['target'][:400]\n",
        "\n",
        "X_test = data['data'][400:]\n",
        "y_test = data['target'][400:]\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# –†–∞—Å—á–µ—Ç r2_score –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "\n",
        "\n",
        "# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "print(\"R2 score –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: {:.2f}\".format(r2_lr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J71riX2Guj3"
      },
      "source": [
        "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (2 –±–∞–ª–ª–∞)\n",
        "\n",
        "–ü–µ—Ä–µ–±–µ—Ä–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–Ω–µ –±–æ–ª–µ–µ –¥–≤—É—Ö-—Ç—Ä—ë—Ö). –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –∫–∞–∫ —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ –æ—à–∏–±–∫—É –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –¥–ª—è –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤–∞ –≥—Ä–∞—Ñ–∏–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (fitting curve) –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π —Ç–æ–º—É, —á—Ç–æ –º—ã —Å—Ç—Ä–æ–∏–ª–∏ –Ω–∞ –∑–∞–Ω—è—Ç–∏–∏. –ù–∞–π–¥–∏—Ç–µ –≥–ª—É–±–∏–Ω—É –¥–µ—Ä–µ–≤–∞, –Ω–∞—á–∏–Ω–∞—è —Å –∫–æ—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def plot_fitting_curve(\n",
        "    model_ctor, parameter: str, values: list, score, X_train, X_test, y_train, y_test\n",
        "):\n",
        "    train_curve = []\n",
        "    test_curve = []\n",
        "    for value in tqdm(values):\n",
        "        model = model_ctor(**{parameter: value}) \n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_train, y_pred_test = model.predict(X_train), model.predict(X_test)\n",
        "        train_curve.append(score(y_train, y_pred_train))\n",
        "        test_curve.append(score(y_test, y_pred_test))\n",
        "    sns.lineplot(x=values, y=train_curve)\n",
        "    sns.lineplot(x=values, y=test_curve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/29 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'Oxford'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_fitting_curve(DecisionTreeRegressor, \u001b[39m'\u001b[39;49m\u001b[39mmax_depth\u001b[39;49m\u001b[39m'\u001b[39;49m, np\u001b[39m.\u001b[39;49marange(\u001b[39m1\u001b[39;49m, \u001b[39m30\u001b[39;49m), r2_score, X_train, X_test, y_train, y_test)\n",
            "Cell \u001b[1;32mIn[58], line 12\u001b[0m, in \u001b[0;36mplot_fitting_curve\u001b[1;34m(model_ctor, parameter, values, score, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m tqdm(values):\n\u001b[0;32m     11\u001b[0m     model \u001b[39m=\u001b[39m model_ctor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{parameter: value}) \n\u001b[1;32m---> 12\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     13\u001b[0m     y_pred_train, y_pred_test \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_train), model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     14\u001b[0m     train_curve\u001b[39m.\u001b[39mappend(score(y_train, y_pred_train))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDecisionTreeRegressor\u001b[39;00m(RegressorMixin, BaseDecisionTree):\n\u001b[0;32m   1035\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A decision tree regressor.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \n\u001b[0;32m   1037\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <tree>`.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \n\u001b[0;32m   1039\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m    criterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\", \\\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[39m            \"poisson\"}, default=\"squared_error\"\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[39m        The function to measure the quality of a split. Supported criteria\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[39m        are \"squared_error\" for the mean squared error, which is equal to\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \u001b[39m        variance reduction as feature selection criterion and minimizes the L2\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[39m        loss using the mean of each terminal node, \"friedman_mse\", which uses\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[39m        mean squared error with Friedman's improvement score for potential\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m        splits, \"absolute_error\" for the mean absolute error, which minimizes\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39m        the L1 loss using the median of each terminal node, and \"poisson\" which\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[39m        uses reduction in Poisson deviance to find splits.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m \u001b[39m        .. versionadded:: 0.18\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[39m           Mean Absolute Error (MAE) criterion.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \n\u001b[0;32m   1055\u001b[0m \u001b[39m        .. versionadded:: 0.24\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[39m            Poisson deviance criterion.\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \n\u001b[0;32m   1058\u001b[0m \u001b[39m        .. deprecated:: 1.0\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[39m            Criterion \"mse\" was deprecated in v1.0 and will be removed in\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[39m            version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \n\u001b[0;32m   1062\u001b[0m \u001b[39m        .. deprecated:: 1.0\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[39m            Criterion \"mae\" was deprecated in v1.0 and will be removed in\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[39m            version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \n\u001b[0;32m   1066\u001b[0m \u001b[39m    splitter : {\"best\", \"random\"}, default=\"best\"\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[39m        The strategy used to choose the split at each node. Supported\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[39m        strategies are \"best\" to choose the best split and \"random\" to choose\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m        the best random split.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \n\u001b[0;32m   1071\u001b[0m \u001b[39m    max_depth : int, default=None\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m        The maximum depth of the tree. If None, then nodes are expanded until\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[39m        all leaves are pure or until all leaves contain less than\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[39m        min_samples_split samples.\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \n\u001b[0;32m   1076\u001b[0m \u001b[39m    min_samples_split : int or float, default=2\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[39m        The minimum number of samples required to split an internal node:\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \n\u001b[0;32m   1079\u001b[0m \u001b[39m        - If int, then consider `min_samples_split` as the minimum number.\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m \u001b[39m        - If float, then `min_samples_split` is a fraction and\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39m          `ceil(min_samples_split * n_samples)` are the minimum\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[39m          number of samples for each split.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \n\u001b[0;32m   1084\u001b[0m \u001b[39m        .. versionchanged:: 0.18\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \u001b[39m           Added float values for fractions.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m \n\u001b[0;32m   1087\u001b[0m \u001b[39m    min_samples_leaf : int or float, default=1\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m \u001b[39m        The minimum number of samples required to be at a leaf node.\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[39m        A split point at any depth will only be considered if it leaves at\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[39m        least ``min_samples_leaf`` training samples in each of the left and\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39m        right branches.  This may have the effect of smoothing the model,\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[39m        especially in regression.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \n\u001b[0;32m   1094\u001b[0m \u001b[39m        - If int, then consider `min_samples_leaf` as the minimum number.\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[39m        - If float, then `min_samples_leaf` is a fraction and\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[39m          `ceil(min_samples_leaf * n_samples)` are the minimum\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39m          number of samples for each node.\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \n\u001b[0;32m   1099\u001b[0m \u001b[39m        .. versionchanged:: 0.18\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39m           Added float values for fractions.\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \n\u001b[0;32m   1102\u001b[0m \u001b[39m    min_weight_fraction_leaf : float, default=0.0\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m \u001b[39m        The minimum weighted fraction of the sum total of weights (of all\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m \u001b[39m        the input samples) required to be at a leaf node. Samples have\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[39m        equal weight when sample_weight is not provided.\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \n\u001b[0;32m   1107\u001b[0m \u001b[39m    max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39m        The number of features to consider when looking for the best split:\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \n\u001b[0;32m   1110\u001b[0m \u001b[39m        - If int, then consider `max_features` features at each split.\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[39m        - If float, then `max_features` is a fraction and\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[39m          `int(max_features * n_features)` features are considered at each\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \u001b[39m          split.\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[39m        - If \"auto\", then `max_features=n_features`.\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39m        - If \"sqrt\", then `max_features=sqrt(n_features)`.\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[39m        - If \"log2\", then `max_features=log2(n_features)`.\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[39m        - If None, then `max_features=n_features`.\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \n\u001b[0;32m   1119\u001b[0m \u001b[39m        Note: the search for a split does not stop until at least one\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m \u001b[39m        valid partition of the node samples is found, even if it requires to\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[39m        effectively inspect more than ``max_features`` features.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m \n\u001b[0;32m   1123\u001b[0m \u001b[39m    random_state : int, RandomState instance or None, default=None\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[39m        Controls the randomness of the estimator. The features are always\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[39m        randomly permuted at each split, even if ``splitter`` is set to\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \u001b[39m        ``\"best\"``. When ``max_features < n_features``, the algorithm will\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m        select ``max_features`` at random at each split before finding the best\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39m        split among them. But the best found split may vary across different\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39m        runs, even if ``max_features=n_features``. That is the case, if the\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[39m        improvement of the criterion is identical for several splits and one\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[39m        split has to be selected at random. To obtain a deterministic behaviour\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[39m        during fitting, ``random_state`` has to be fixed to an integer.\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[39m        See :term:`Glossary <random_state>` for details.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \n\u001b[0;32m   1135\u001b[0m \u001b[39m    max_leaf_nodes : int, default=None\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[39m        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[39m        Best nodes are defined as relative reduction in impurity.\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[39m        If None then unlimited number of leaf nodes.\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \n\u001b[0;32m   1140\u001b[0m \u001b[39m    min_impurity_decrease : float, default=0.0\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[39m        A node will be split if this split induces a decrease of the impurity\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[39m        greater than or equal to this value.\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \n\u001b[0;32m   1144\u001b[0m \u001b[39m        The weighted impurity decrease equation is the following::\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \n\u001b[0;32m   1146\u001b[0m \u001b[39m            N_t / N * (impurity - N_t_R / N_t * right_impurity\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[39m                                - N_t_L / N_t * left_impurity)\u001b[39;00m\n\u001b[0;32m   1148\u001b[0m \n\u001b[0;32m   1149\u001b[0m \u001b[39m        where ``N`` is the total number of samples, ``N_t`` is the number of\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m \u001b[39m        samples at the current node, ``N_t_L`` is the number of samples in the\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m \u001b[39m        left child, and ``N_t_R`` is the number of samples in the right child.\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m \n\u001b[0;32m   1153\u001b[0m \u001b[39m        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m \u001b[39m        if ``sample_weight`` is passed.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \n\u001b[0;32m   1156\u001b[0m \u001b[39m        .. versionadded:: 0.19\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[0;32m   1158\u001b[0m \u001b[39m    ccp_alpha : non-negative float, default=0.0\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m \u001b[39m        Complexity parameter used for Minimal Cost-Complexity Pruning. The\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[39m        subtree with the largest cost complexity that is smaller than\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m \u001b[39m        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m \u001b[39m        :ref:`minimal_cost_complexity_pruning` for details.\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m \n\u001b[0;32m   1164\u001b[0m \u001b[39m        .. versionadded:: 0.22\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m \n\u001b[0;32m   1166\u001b[0m \u001b[39m    Attributes\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[39m    feature_importances_ : ndarray of shape (n_features,)\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[39m        The feature importances.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \u001b[39m        The higher, the more important the feature.\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m \u001b[39m        The importance of a feature is computed as the\u001b[39;00m\n\u001b[0;32m   1172\u001b[0m \u001b[39m        (normalized) total reduction of the criterion brought\u001b[39;00m\n\u001b[0;32m   1173\u001b[0m \u001b[39m        by that feature. It is also known as the Gini importance [4]_.\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m \n\u001b[0;32m   1175\u001b[0m \u001b[39m        Warning: impurity-based feature importances can be misleading for\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[39m        high cardinality features (many unique values). See\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[39m        :func:`sklearn.inspection.permutation_importance` as an alternative.\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m \n\u001b[0;32m   1179\u001b[0m \u001b[39m    max_features_ : int\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m \u001b[39m        The inferred value of max_features.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \n\u001b[0;32m   1182\u001b[0m \u001b[39m    n_features_ : int\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[39m        The number of features when ``fit`` is performed.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \n\u001b[0;32m   1185\u001b[0m \u001b[39m        .. deprecated:: 1.0\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m \u001b[39m           `n_features_` is deprecated in 1.0 and will be removed in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m           1.2. Use `n_features_in_` instead.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \n\u001b[0;32m   1189\u001b[0m \u001b[39m    n_features_in_ : int\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39m        Number of features seen during :term:`fit`.\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \n\u001b[0;32m   1192\u001b[0m \u001b[39m        .. versionadded:: 0.24\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \n\u001b[0;32m   1194\u001b[0m \u001b[39m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \u001b[39m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m \u001b[39m        has feature names that are all strings.\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m \n\u001b[0;32m   1198\u001b[0m \u001b[39m        .. versionadded:: 1.0\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \n\u001b[0;32m   1200\u001b[0m \u001b[39m    n_outputs_ : int\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[39m        The number of outputs when ``fit`` is performed.\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \n\u001b[0;32m   1203\u001b[0m \u001b[39m    tree_ : Tree instance\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[39m        The underlying Tree object. Please refer to\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[39m        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m \u001b[39m        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\u001b[39;00m\n\u001b[0;32m   1207\u001b[0m \u001b[39m        for basic usage of these attributes.\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \n\u001b[0;32m   1209\u001b[0m \u001b[39m    See Also\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m \u001b[39m    --------\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \u001b[39m    DecisionTreeClassifier : A decision tree classifier.\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m \n\u001b[0;32m   1213\u001b[0m \u001b[39m    Notes\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m    -----\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m    The default values for the parameters controlling the size of the trees\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m    unpruned trees which can potentially be very large on some data sets. To\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[39m    reduce memory consumption, the complexity and size of the trees should be\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[39m    controlled by setting those parameter values.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[39m    References\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m \n\u001b[0;32m   1224\u001b[0m \u001b[39m    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m \n\u001b[0;32m   1226\u001b[0m \u001b[39m    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[39m           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m \n\u001b[0;32m   1229\u001b[0m \u001b[39m    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \u001b[39m           Learning\", Springer, 2009.\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m \n\u001b[0;32m   1232\u001b[0m \u001b[39m    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[39m           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\u001b[39;00m\n\u001b[0;32m   1234\u001b[0m \n\u001b[0;32m   1235\u001b[0m \u001b[39m    Examples\u001b[39;00m\n\u001b[0;32m   1236\u001b[0m \u001b[39m    --------\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[39m    >>> from sklearn.datasets import load_diabetes\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[39m    >>> from sklearn.model_selection import cross_val_score\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[39m    >>> from sklearn.tree import DecisionTreeRegressor\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[39m    >>> X, y = load_diabetes(return_X_y=True)\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m \u001b[39m    >>> regressor = DecisionTreeRegressor(random_state=0)\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m \u001b[39m    >>> cross_val_score(regressor, X, y, cv=10)\u001b[39;00m\n\u001b[0;32m   1243\u001b[0m \u001b[39m    ...                    # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \u001b[39m    ...\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[39m    array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[39m           0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m   1250\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m   1251\u001b[0m         \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1262\u001b[0m         ccp_alpha\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m,\n\u001b[0;32m   1263\u001b[0m     ):\n\u001b[0;32m   1264\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m   1265\u001b[0m             criterion\u001b[39m=\u001b[39mcriterion,\n\u001b[0;32m   1266\u001b[0m             splitter\u001b[39m=\u001b[39msplitter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1275\u001b[0m             ccp_alpha\u001b[39m=\u001b[39mccp_alpha,\n\u001b[0;32m   1276\u001b[0m         )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:186\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mSome value(s) of y are negative which is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m not allowed for Poisson regression.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m             )\n\u001b[0;32m    182\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39msum(y) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    184\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mSum of y is not positive which is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mnecessary for Poisson regression.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 186\u001b[0m             )\n\u001b[0;32m    188\u001b[0m \u001b[39m# Determine output settings\u001b[39;00m\n\u001b[0;32m    189\u001b[0m n_samples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:579\u001b[0m, in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    577\u001b[0m     check_X_params, check_y_params \u001b[39m=\u001b[39m validate_separately\n\u001b[0;32m    578\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params)\n\u001b[1;32m--> 579\u001b[0m     y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:1998\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1997\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1998\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(values, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1999\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2000\u001b[0m         astype_is_view(values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2001\u001b[0m         \u001b[39mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2002\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mis_single_block\n\u001b[0;32m   2003\u001b[0m     ):\n\u001b[0;32m   2004\u001b[0m         \u001b[39m# Check if both conversions can be done without a copy\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m         \u001b[39mif\u001b[39;00m astype_is_view(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m astype_is_view(\n\u001b[0;32m   2006\u001b[0m             values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype\n\u001b[0;32m   2007\u001b[0m         ):\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Oxford'"
          ]
        }
      ],
      "source": [
        "plot_fitting_curve(DecisionTreeRegressor, 'max_depth', np.arange(1, 30), r2_score, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJlf6gJ9RBhf"
      },
      "source": [
        "# –ü—Ä–æ—Å—Ç–æ–µ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (1 –±–∞–ª–ª)\n",
        "\n",
        "–í —ç—Ç–æ–π —Å–µ–∫—Ü–∏–∏ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∞–Ω—Å–∞–º–±–ª—å –¥–µ—Ä–µ–≤—å–µ–≤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht7wq9TqUfvZ"
      },
      "outputs": [],
      "source": [
        "class EnsembleTreeRegressor:\n",
        "    def __init__(self, num_trees=5, samples_frac=0.8, **model_kwargs):\n",
        "        self.num_trees= num_trees\n",
        "        self._samples_frac = 0.8\n",
        "        self._trees = [DecisionTreeRegressor(**model_kwargs) for _ in range(num_trees)]\n",
        "    def fit(self, x, y: pd.Series):\n",
        "        x = pd.DataFrame(x)\n",
        "        y = y.reset_index(drop=True)\n",
        "        for tree in self._trees:\n",
        "            tree_x = x.sample(frac=self._samples_frac, random_state=42)\n",
        "            tree_y = y[tree_x.index]\n",
        "            tree.fit(tree_x, tree_y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, x: pd.DataFrame):\n",
        "        x = pd.DataFrame(x)\n",
        "        res = []\n",
        "        for i in range(self.num_trees):\n",
        "          res.append(self._trees[i].predict(x))\n",
        "        return sum(res) / len(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCxogZPuVZCF"
      },
      "source": [
        "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —ç—Ç–æ—Ç –∞–Ω—Å–∞–º–±–ª—å –ª—É—á—à–µ –æ–±—ã—á–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é?\n",
        "\n",
        "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–±–µ—Ä–∏—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É –¥–µ—Ä–µ–≤–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –º–æ–º–µ–Ω—Ç –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —É –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞ –∏ —É –∞–Ω—Å–∞–º–±–ª—è. –ó–∞–≤–∏—Å–∏—Ç –ª–∏ —ç—Ç–æ—Ç –º–æ–º–µ–Ω—Ç –æ—Ç —á–∏—Å–ª–∞ –¥–µ—Ä–µ–≤—å–µ–≤ (`num_trees`)? –û—Ç —á–∏—Å–ª–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ (`samples_frac`)? –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ fitting curve."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
